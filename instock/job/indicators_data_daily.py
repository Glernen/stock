#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# åœ¨é¡¹ç›®è¿è¡Œæ—¶ï¼Œä¸´æ—¶å°†é¡¹ç›®è·¯å¾„æ·»åŠ åˆ°ç¯å¢ƒå˜é‡
import os.path
import sys
cpath_current = os.path.dirname(os.path.dirname(__file__))
cpath = os.path.abspath(os.path.join(cpath_current, os.pardir))
sys.path.append(cpath)

import pandas as pd
import numpy as np
import talib as tl
import mysql.connector
import datetime
import threading
from typing import Optional  # æ–°å¢å¯¼å…¥
# from datetime import datetime, timedelta
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple
from mysql.connector import Error
# from instock.lib.database import DBManager
import sqlalchemy
import time
import instock.core.tablestructure as tbs
from instock.lib.database import db_host, db_user, db_password, db_database, db_charset



def calculate_indicators(data):
    # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦æ»¡è¶³æœ€å°çª—å£ï¼ˆä¾‹å¦‚MACDéœ€è¦è‡³å°‘34æ¡æ•°æ®ï¼‰
    min_window = 34  # æ ¹æ®TA-LibæŒ‡æ ‡è¦æ±‚è°ƒæ•´
    if len(data) < min_window:
        print(f"[WARNING] æ•°æ®ä¸è¶³{min_window}æ¡ï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡")
        return pd.DataFrame()

    daily_data_indicators = pd.DataFrame()
    data = data.sort_values(by='date', ascending=True)
   # åŸå§‹å­—æ®µ
    daily_data_indicators['date'] = data['date']
    daily_data_indicators['code'] = data['code']


    # æ ¼å¼åŒ–æ—¥æœŸä¸º YYYYMMDD
    daily_data_indicators['date_int'] = data['date'].astype(str).str.replace('-', '')

    if 'code_int' in data.columns:
        daily_data_indicators['code_int'] = data['code_int']
    if 'name' in data.columns:
        daily_data_indicators['name'] = data['name']
    daily_data_indicators['close'] = data['close']
    '''
    è®¡ç®—ETFæ•°æ®çš„å„ç§æŒ‡æ ‡ã€‚

    å‚æ•°:
    data (pd.DataFrame): åŒ…å«ETFæ¯æ—¥æ•°æ®çš„DataFrameï¼Œè‡³å°‘åŒ…å«'open' ,'close', 'high', 'low', 'volume'åˆ— æ”¶ç›˜ä»·ï¼Œæœ€é«˜ä»·ï¼Œæœ€ä½ä»·ï¼Œæˆäº¤é‡

    è¿”å›:
    pd.DataFrame: åŒ…å«è®¡ç®—åæŒ‡æ ‡æ•°æ®çš„DataFrame
    '''


    # è®¡ç®—MACD
    daily_data_indicators['macd'], daily_data_indicators['macds'], daily_data_indicators['macdh'] = tl.MACD(data['close'])

    # è®¡ç®—KDJçš„Kå’ŒD
    daily_data_indicators['kdjk'], daily_data_indicators['kdjd'] = tl.STOCH(
        data['high'],
        data['low'],
        data['close'],
        fastk_period=9,    # é»˜è®¤å‚æ•°éœ€æ˜¾å¼æŒ‡å®š
        slowk_period=5,
        slowk_matype=1,
        slowd_period=5,
        slowd_matype=1
    )

    # æ‰‹åŠ¨è®¡ç®—Jçº¿ï¼ˆJ = 3*K - 2*Dï¼‰
    daily_data_indicators['kdjj'] = 3 * daily_data_indicators['kdjk'] - 2 * daily_data_indicators['kdjd']

    # è®¡ç®—BOLL
    daily_data_indicators['boll_ub'], daily_data_indicators['boll'], daily_data_indicators['boll_lb'] = tl.BBANDS(data['close'])

    # è®¡ç®—W&R
    daily_data_indicators['wr_6'] = tl.WILLR(data['high'], data['low'], data['close'], timeperiod=6)
    daily_data_indicators['wr_10'] = tl.WILLR(data['high'], data['low'], data['close'], timeperiod=10)
    daily_data_indicators['wr_14'] = tl.WILLR(data['high'], data['low'], data['close'], timeperiod=14)

    # è®¡ç®—CCI
    daily_data_indicators['cci'] = tl.CCI(data['high'], data['low'], data['close'])
    daily_data_indicators['cci_84'] = tl.SMA(daily_data_indicators['cci'], timeperiod=84)

    # # è®¡ç®—TRIXå’ŒTRMAï¼ˆå‡è®¾TRMAæ˜¯TRIXçš„ç®€å•ç§»åŠ¨å¹³å‡ï¼‰
    # daily_data_indicators['trix'] = tl.TRIX(data['close'])
    # daily_data_indicators['trix_20_sma'] = tl.SMA(daily_data_indicators['trix'], timeperiod=20)

    # è®¡ç®—CR
    # data['m_price'] = data['amount'] / data['volume']
    # data['m_price_sf1'] = data['m_price'].shift(1, fill_value=0.0)
    # data['h_m'] = data['high'] - data[['m_price_sf1', 'high']].min(axis=1)
    # data['m_l'] = data['m_price_sf1'] - data[['m_price_sf1', 'low']].min(axis=1)
    # data['h_m_sum'] = data['h_m'].rolling(window=26).sum()
    # data['m_l_sum'] = data['m_l'].rolling(window=26).sum()
    # data['cr'] = (data['h_m_sum'] / data['m_l_sum']).fillna(0).replace([np.inf, -np.inf], 0) * 100
    # data['cr-ma1'] = data['cr'].rolling(window=5).mean()
    # data['cr-ma2'] = data['cr'].rolling(window=10).mean()
    # data['cr-ma3'] = data['cr'].rolling(window=20).mean()

    # è®¡ç®—SMAï¼ˆç®€å•ç§»åŠ¨å¹³å‡ï¼‰
    # data['sma'] = tl.SMA(data['close'])

    # è®¡ç®—RSI
    daily_data_indicators['rsi_6'] = tl.RSI(data['close'], timeperiod=6)
    daily_data_indicators['rsi_12'] = tl.RSI(data['close'], timeperiod=12)
    daily_data_indicators['rsi'] = tl.RSI(data['close'])
    daily_data_indicators['rsi_24'] = tl.RSI(data['close'], timeperiod=24)

    # æ‰‹åŠ¨è®¡ç®—VR
    close_diff = data['close'].diff()
    up_volume = data['volume'] * (close_diff > 0).astype(int)
    down_volume = data['volume'] * (close_diff < 0).astype(int)
    daily_data_indicators['vr'] = up_volume.rolling(window=26).sum() / down_volume.rolling(window=26).sum() * 100
    daily_data_indicators['vr'] = daily_data_indicators['vr'].fillna(0.0).replace([np.inf, -np.inf], 0.0)
    daily_data_indicators['vr_6_sma'] = tl.SMA(daily_data_indicators['vr'], timeperiod=6)

    # è®¡ç®—ROC
    # ä¿®æ­£ï¼šROCå‡½æ•°è¿”å›å€¼åªæœ‰ä¸€ä¸ªï¼ŒåŸä»£ç å¯èƒ½æœ‰è¯¯
    daily_data_indicators['roc'] = tl.ROC(data['close'])

     # è®¡ç®—DMIç›¸å…³æŒ‡æ ‡
    timeperiod = 14
    daily_data_indicators['pdi'] = tl.PLUS_DI(data['high'], data['low'], data['close'], timeperiod=timeperiod)
    daily_data_indicators['mdi'] = tl.MINUS_DI(data['high'], data['low'], data['close'], timeperiod=timeperiod)
    daily_data_indicators['dx'] = tl.DX(data['high'], data['low'], data['close'], timeperiod=timeperiod)
    daily_data_indicators['adx'] = tl.ADX(data['high'], data['low'], data['close'], timeperiod=timeperiod)
    daily_data_indicators['adxr'] = tl.ADXR(data['high'], data['low'], data['close'], timeperiod=timeperiod)


    # è®¡ç®—TRå’ŒATR
    daily_data_indicators['tr'] = tl.TRANGE(data['high'], data['low'], data['close'])
    daily_data_indicators['atr'] = tl.ATR(data['high'], data['low'], data['close'])

    # è®¡ç®—DMAå’ŒAMAï¼ˆå‡è®¾AMAæ˜¯DMAçš„ç®€å•ç§»åŠ¨å¹³å‡ï¼‰
    # data['dma'] = tl.DMA(data['close'])
    # data['dma_10_sma'] = tl.SMA(data['dma'], timeperiod=10)

    # è®¡ç®—OBV
    daily_data_indicators['obv'] = tl.OBV(data['close'], data['volume'])

    # è®¡ç®—SAR
    daily_data_indicators['sar'] = tl.SAR(data['high'], data['low'])

    # è®¡ç®—PSY
    price_up = (data['close'] > data['close'].shift(1)).astype(int)
    daily_data_indicators['psy'] = (price_up.rolling(12).sum() / 12 * 100).fillna(0)
    daily_data_indicators['psyma'] = daily_data_indicators['psy'].rolling(6).mean()

    # è®¡ç®—BRAR
    prev_close = data['close'].shift(1, fill_value=0)
    br_up = (data['high'] - prev_close).clip(lower=0)
    br_down = (prev_close - data['low']).clip(lower=0)
    daily_data_indicators['br'] = (br_up.rolling(26).sum() / br_down.rolling(26).sum()).fillna(0).replace([np.inf, -np.inf], 0) * 100

    ar_up = (data['high'] - data['open']).clip(lower=0)
    ar_down = (data['open'] - data['low']).clip(lower=0)
    daily_data_indicators['ar'] = (ar_up.rolling(26).sum() / ar_down.rolling(26).sum()).fillna(0).replace([np.inf, -np.inf], 0) * 100

    # è®¡ç®—EMV
    hl_avg = (data['high'] + data['low']) / 2
    prev_hl_avg = hl_avg.shift(1, fill_value=0)
    volume = data['volume'].replace(0, 1)  # é¿å…é™¤é›¶
    daily_data_indicators['emv'] = ((hl_avg - prev_hl_avg) * (data['high'] - data['low']) / volume).rolling(14).sum()
    daily_data_indicators['emva'] = daily_data_indicators['emv'].rolling(9).mean()

    # # è®¡ç®—BIAS
    # daily_data_indicators['bias'] = (data['close'] - tl.SMA(data['close'], timeperiod=6)) / tl.SMA(data['close'], timeperiod=6) * 100
    # daily_data_indicators['bias_12'] = (data['close'] - tl.SMA(data['close'], timeperiod=12)) / tl.SMA(data['close'], timeperiod=12) * 100
    # daily_data_indicators['bias_24'] = (data['close'] - tl.SMA(data['close'], timeperiod=24)) / tl.SMA(data['close'], timeperiod=24) * 100

    # è®¡ç®—MFI
    daily_data_indicators['mfi'] = tl.MFI(data['high'], data['low'], data['close'], data['volume'])
    daily_data_indicators['mfisma'] = tl.SMA(daily_data_indicators['mfi'])

    # è®¡ç®—VWMA
    daily_data_indicators['vwma'] = (data['close'] * data['volume']).cumsum() / data['volume'].cumsum()
    daily_data_indicators['mvwma'] = tl.SMA(daily_data_indicators['vwma'])

    # è®¡ç®—PPO
    daily_data_indicators['ppo'] = tl.PPO(data['close'], fastperiod=12, slowperiod=26, matype=1)
    daily_data_indicators['ppos'] = tl.EMA(daily_data_indicators['ppo'], timeperiod=9)
    daily_data_indicators['ppoh'] = daily_data_indicators['ppo'] - daily_data_indicators['ppos']
    daily_data_indicators['ppo'] = daily_data_indicators['ppo'].fillna(0)
    daily_data_indicators['ppos'] = daily_data_indicators['ppos'].fillna(0)
    daily_data_indicators['ppoh'] = daily_data_indicators['ppoh'].fillna(0)

    # è®¡ç®—WTï¼ˆå‡è®¾WT1å’ŒWT2çš„è®¡ç®—æ–¹æ³•ï¼‰
    daily_data_indicators['wt1'] = (data['close'] - tl.SMA(data['close'], timeperiod=10)) / tl.STDDEV(data['close'], timeperiod=10)
    daily_data_indicators['wt2'] = (data['close'] - tl.SMA(data['close'], timeperiod=20)) / tl.STDDEV(data['close'], timeperiod=20)

    # è®¡ç®—Supertrendï¼ˆç®€å•ç¤ºä¾‹ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„å®ç°ï¼‰
    # atr_multiplier = 3
    # daily_data_indicators['atr'] = tl.ATR(data['high'], data['low'], data['close'])
    # daily_data_indicators['upper_band'] = data['close'] + (atr_multiplier * daily_data_indicators['atr'])
    # daily_data_indicators['lower_band'] = data['close'] - (atr_multiplier * daily_data_indicators['atr'])
    # daily_data_indicators['supertrend'] = daily_data_indicators['upper_band']
    # daily_data_indicators['supertrend_ub'] = daily_data_indicators['upper_band']
    # daily_data_indicators['supertrend_lb'] = daily_data_indicators['lower_band']

    # è®¡ç®—DPO
    daily_data_indicators['dpo'] = data['close'] - tl.SMA(data['close'], timeperiod=20)
    daily_data_indicators['madpo'] = tl.SMA(daily_data_indicators['dpo'])

    # è®¡ç®—VHF
    window = 28
    high_close = data['close'].rolling(window).max()
    low_close = data['close'].rolling(window).min()
    sum_diff = abs(data['close'] - data['close'].shift(1)).rolling(window).sum()
    daily_data_indicators['vhf'] = ((high_close - low_close) / sum_diff).fillna(0)

    # è®¡ç®—RVI
    rvi_x = (
        (data['close'] - data['open']) +
        2 * (data['close'].shift(1) - data['open'].shift(1)) +
        2 * (data['close'].shift(2) - data['open'].shift(2)) +
        (data['close'].shift(3) - data['open'].shift(3))
    ) / 6

    rvi_y = (
        (data['high'] - data['low']) +
        2 * (data['high'].shift(1) - data['low'].shift(1)) +
        2 * (data['high'].shift(2) - data['low'].shift(2)) +
        (data['high'].shift(3) - data['low'].shift(3))
    ) / 6

    daily_data_indicators['rvi'] = (rvi_x.rolling(10).mean() / rvi_y.rolling(10).mean()).fillna(0)
    daily_data_indicators['rvis'] = (
        daily_data_indicators['rvi'] +
        2 * daily_data_indicators['rvi'].shift(1) +
        2 * daily_data_indicators['rvi'].shift(2) +
        daily_data_indicators['rvi'].shift(3)
    ) / 6

    # è®¡ç®—FI
    daily_data_indicators['fi'] = (data['close'] - data['close'].shift(1)) * data['volume']
    daily_data_indicators['force_2'] = tl.SMA(daily_data_indicators['fi'], timeperiod=2)
    daily_data_indicators['force_13'] = tl.SMA(daily_data_indicators['fi'], timeperiod=13)

    # è®¡ç®—ENE
    daily_data_indicators['ene_ue'] = tl.EMA(data['close'], timeperiod=25) + 2 * tl.STDDEV(data['close'], timeperiod=25)
    daily_data_indicators['ene'] = tl.EMA(data['close'], timeperiod=25)
    daily_data_indicators['ene_le'] = tl.EMA(data['close'], timeperiod=25) - 2 * tl.STDDEV(data['close'], timeperiod=25)

    # è®¡ç®—STOCHRSI
    daily_data_indicators['stochrsi_k'], daily_data_indicators['stochrsi_d'] = tl.STOCHRSI(data['close'])
    daily_data_indicators = daily_data_indicators.replace([np.inf, -np.inf], np.nan)
    daily_data_indicators = daily_data_indicators.fillna(0)

    # print(f'{daily_data_indicators}')

    return daily_data_indicators







# è¡¨æ˜ å°„é…ç½®
TABLE_MAP = {
    'stock': {
        'hist_table': tbs.CN_STOCK_HIST_DAILY_DATA['name'],
        'info_table': tbs.TABLE_STOCK_INIT['name']
    },
    'etf': {
        'hist_table': tbs.CN_ETF_HIST_DAILY_DATA['name'],
        'info_table': tbs.TABLE_ETF_INIT['name']
    },
    'index': {
        'hist_table': tbs.CN_INDEX_HIST_DAILY_DATA['name'],
        'info_table': tbs.TABLE_INDEX_INIT['name']
    }
}

INDICATOR_TABLES = {
    'stock': 'cn_stock_indicators',
    'etf': 'cn_etf_indicators',
    'index': 'cn_index_indicators'
}

# æ•°æ®åº“è¿æ¥é…ç½®
MAX_HISTORY_WINDOW = 200  # æŒ‡æ ‡è®¡ç®—æ‰€éœ€æœ€å¤§å†å²çª—å£


def get_latest_codes(data_type: str) -> List[str]:
    """è·å–æŒ‡å®šç±»å‹çš„æœ€æ–°ä»£ç åˆ—è¡¨"""
    try:
        with DBManager.get_new_connection() as conn:
            query = f"""
                SELECT code_int
                FROM {TABLE_MAP[data_type]['info_table']}
                WHERE date = (SELECT MAX(date) FROM {TABLE_MAP[data_type]['info_table']})
            """
            return pd.read_sql(query, conn)['code_int'].tolist()
    except Exception as e:
        print(f"è·å–{data_type}ä»£ç å¤±è´¥ï¼š{str(e)}")
        return []

def get_hist_data(code: int, data_type: str, last_date: str = None) -> pd.DataFrame:
    """è·å–å¸¦æ—¥æœŸèŒƒå›´çš„è¡Œæƒ…æ•°æ®"""
    try:
        with DBManager.get_new_connection() as conn:
            base_query = f"""
                SELECT * FROM {TABLE_MAP[data_type]['hist_table']}
                WHERE code_int = '{code}'
            """

            if last_date:
                query = f"""
                    {base_query}
                    AND date >= (
                        SELECT DATE_SUB('{last_date}', INTERVAL {MAX_HISTORY_WINDOW} DAY)
                        FROM DUAL
                    )
                """
            else:
                query = base_query + " ORDER BY date ASC LIMIT 1000"

            data = pd.read_sql(query, conn)
            # --- è°ƒè¯•7: éªŒè¯åŸå§‹æ•°æ®è´¨é‡ ---
            # print(f"[DEBUG] {code} åŸå§‹æ•°æ®ç»Ÿè®¡:")
            # print("è®°å½•æ•°:", len(data))
            # print("æ—¶é—´èŒƒå›´:", data['date'].min(), "è‡³", data['date'].max())
            # print("ç¼ºå¤±å€¼ç»Ÿè®¡:")
            # print(data[['close', 'high', 'low', 'volume']].isnull().sum())

            return data.sort_values('date', ascending=True) if not data.empty else pd.DataFrame()
            # return data.sort_values('date', ascending=True)
    except Exception as e:
        print(f"è·å–{data_type}å†å²æ•°æ®å¤±è´¥ï¼š{code_int}-{str(e)}")
        return pd.DataFrame()

def calculate_and_save(code: str, data_type: str):
    """å®Œæ•´çš„å¤„ç†æµæ°´çº¿"""
    try:
        # æ£€æŸ¥ç›®æ ‡è¡¨æ˜¯å¦å­˜åœ¨
        table_name = INDICATOR_TABLES[data_type]
        # create_table_if_not_exists(table_name)

        # è·å–æœ€æ–°å¤„ç†æ—¥æœŸ
        last_processed_date = get_last_processed_date(table_name, code)

        # è·å–å†å²æ•°æ®ï¼ˆå¢é‡é€»è¾‘ï¼‰
        hist_data = get_hist_data(code, data_type, last_processed_date)
        if hist_data.empty:
            print(f"è·³è¿‡ç©ºæ•°æ®ï¼š{data_type} {code}")
            return

        # æ£€æŸ¥å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨
        required_columns = {'date', 'code','open', 'close', 'high', 'low', 'volume'}
        missing_columns = required_columns - set(hist_data.columns)
        if missing_columns:
            print(f"æ•°æ®ç¼ºå¤±å…³é”®åˆ— {missing_columns}ï¼Œè·³è¿‡å¤„ç†ï¼š{code}")
            return

        # è®¡ç®—æŒ‡æ ‡
        # print(f"\n=== å¼€å§‹å¤„ç† {code} ===")
        indicators = calculate_indicators(hist_data)

        # --- è°ƒè¯•5: è¾“å‡ºå‰5è¡Œæ•°æ®æ ·æœ¬ ---
        # print(f"[DEBUG] {code} è®¡ç®—ç»“æœæ ·æœ¬:")
        # print(indicators.head())

        # --- è°ƒè¯•6: æ£€æŸ¥æ˜¯å¦å­˜åœ¨è´Ÿæ— ç©·æˆ–é›¶å€¼ ---
        # print(f"[DEBUG] {code} å¼‚å¸¸å€¼ç»Ÿè®¡:")
        # print("Inf values:", (indicators == np.inf).sum().sum())
        # print("-Inf values:", (indicators == -np.inf).sum().sum())
        # print("Zero values:", (indicators == 0).sum().sum())

        # è¿‡æ»¤å·²å­˜åœ¨æ•°æ®
        if last_processed_date:
            indicators = indicators[indicators['date'] > last_processed_date]

        # å†™å…¥æ•°æ®åº“å‰æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
        if not indicators.empty:
            # æ–°å¢è¿‡æ»¤æ¡ä»¶ï¼šåˆ é™¤ cci_84 ä¸º0çš„è¡Œ
            if 'cci_84' in indicators.columns:
                indicators = indicators[indicators['cci_84'] != 0]
            if not indicators.empty:
                sync_and_save(table_name, indicators)
                print(f"æ›´æ–°{data_type}æŒ‡æ ‡ï¼š{code} {len(indicators)}æ¡")
        else:
            print(f"æ— æ–°æ•°æ®éœ€æ›´æ–°ï¼š{code}")
    except Exception as e:
        print(f"å¤„ç†{data_type} {code}å¤±è´¥ï¼š{str(e)}")

def get_latest_codes(data_type: str) -> List[int]:
    """è·å–æŒ‡å®šç±»å‹çš„æœ€æ–°ä»£ç åˆ—è¡¨ï¼ˆè¿”å›æ•´æ•°åˆ—è¡¨ï¼‰"""
    try:
        with DBManager.get_new_connection() as conn:
            query = f"""
                SELECT code_int
                FROM {TABLE_MAP[data_type]['info_table']}
                WHERE date = (SELECT MAX(date) FROM {TABLE_MAP[data_type]['info_table']})
            """
            df = pd.read_sql(query, conn)
            return df['code_int'].astype(int).tolist()  # å¼ºåˆ¶è½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨
    except Exception as e:
        print(f"è·å–{data_type}ä»£ç å¤±è´¥ï¼š{str(e)}")
        return []

def get_last_processed_date(table: str, code: int) -> str:
    """è·å–æŒ‡å®šä»£ç çš„æœ€åå¤„ç†æ—¥æœŸ"""
    try:
        with DBManager.get_new_connection() as conn:
            query = f"""
                SELECT MAX(date) AS last_date
                FROM {table}
                WHERE code_int = '{code}'
            """
            result = pd.read_sql(query, conn)
            return result.iloc[0]['last_date']
    except:
        return None


def get_last_processed_dates_batch(table: str, codes: List[int]) -> Dict[int, str]:
    """æ‰¹é‡è·å–å¤šä¸ªä»£ç çš„æœ€åå¤„ç†æ—¥æœŸ"""
    if not codes:
        return {}

    try:
        with DBManager.get_new_connection() as conn:
            code_list = ",".join(map(str, codes))
            query = f"""
                SELECT code_int, MAX(date) AS last_date
                FROM {table}
                WHERE code_int IN ({code_list})
                GROUP BY code_int
            """
            df = pd.read_sql(query, conn)
            return df.set_index('code_int')['last_date'].to_dict()
    except Exception as e:
        print(f"è·å–æœ€åå¤„ç†æ—¥æœŸå¤±è´¥ï¼š{str(e)}")
        return {}


def sync_and_save(table_name: str, data: pd.DataFrame):
    # print(f"[DEBUG] å‡†å¤‡å†™å…¥æ•°æ®ï¼Œå½¢çŠ¶ï¼š{data.shape}")
    """åŒæ­¥è¡¨ç»“æ„å¹¶ä¿å­˜æ•°æ®"""
    # with DBManager.get_new_connection() as conn:
    #     try:
    #         åŒæ­¥è¡¨ç»“æ„(conn, table_name, data.columns)
    #     finally:
    #         if conn.is_connected():
    #             conn.close()

    sql_txt = sqlè¯­å¥ç”Ÿæˆå™¨(table_name, data)
    execute_raw_sql(sql_txt)


class DBManager:
    @staticmethod
    def get_new_connection():
        """åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªæ–°çš„æ•°æ®åº“è¿æ¥"""
        try:
            connection = mysql.connector.connect(
                host=db_host,
                user=db_user,
                password=db_password,
                database=db_database,
                charset=db_charset
            )
            return connection
        except Error as e:
            print(f"Error while connecting to MySQL: {e}")
            return None

    @staticmethod
    def execute_sql(sql: str, params=None):
        """å®‰å…¨æ‰§è¡Œ SQL è¯­å¥"""
        connection = DBManager.get_new_connection()
        if connection:
            try:
                cursor = connection.cursor(buffered=True)
                cursor.execute(sql, params)
                connection.commit()
                cursor.close()
            except Error as e:
                print(f"Error while executing SQL: {e}")
            finally:
                if connection.is_connected():
                    connection.close()


def create_table_if_not_exists(table_name):
    # åˆ›å»ºè¡¨ï¼ˆä¸å«ç´¢å¼•ï¼‰
    create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS `{table_name}` (
            `id` INT AUTO_INCREMENT PRIMARY KEY,
            `date` DATE,
            `date_int` INT,
            `code_int` INT,
            `code` VARCHAR(6),
            `name` VARCHAR(20)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;
    """
    DBManager.execute_sql(create_table_sql)

    # æ£€æŸ¥å¹¶æ·»åŠ idåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    conn = DBManager.get_new_connection()
    if conn:
        try:
            cursor = conn.cursor()
            # æ£€æŸ¥idåˆ—æ˜¯å¦å­˜åœ¨
            check_sql = f"""
                SELECT COUNT(*)
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = DATABASE()
                  AND TABLE_NAME = '{table_name}'
                  AND COLUMN_NAME = 'id';
            """
            cursor.execute(check_sql)
            count = cursor.fetchone()[0]
            if count == 0:
                # æ·»åŠ idåˆ—
                alter_sql = f"""
                    ALTER TABLE `{table_name}`
                    ADD COLUMN `id` INT AUTO_INCREMENT PRIMARY KEY FIRST;
                """
                cursor.execute(alter_sql)
                conn.commit()
                print(f"è¡¨ {table_name} æˆåŠŸæ·»åŠ  id åˆ—")
        except Error as e:
            print(f"æ£€æŸ¥æˆ–æ·»åŠ  id åˆ—å¤±è´¥: {e}")
            conn.rollback()
        finally:
            if conn.is_connected():
                cursor.close()
                conn.close()

    # åˆ›å»ºç´¢å¼•ï¼ˆä¿®å¤ Unread result found é—®é¢˜ï¼‰
    def create_index(index_name, columns, is_unique=False):
        conn = DBManager.get_new_connection()
        try:
            cursor = conn.cursor()
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            check_sql = f"""
                SELECT COUNT(*)
                FROM information_schema.STATISTICS
                WHERE table_name = '{table_name}'
                  AND index_name = '{index_name}';
            """
            cursor.execute(check_sql)
            result = cursor.fetchall()  # å¼ºåˆ¶è¯»å–ç»“æœ
            if result[0][0] == 0:
                index_type = "UNIQUE" if is_unique else ""
                create_sql = f"""
                    CREATE {index_type} INDEX `{index_name}`
                    ON `{table_name}` ({', '.join(columns)});
                """
                cursor.execute(create_sql)
                conn.commit()
        except Error as e:
            print(f"åˆ›å»ºç´¢å¼• {index_name} å¤±è´¥: {e}")
            conn.rollback()
        finally:
            if conn.is_connected():
                cursor.close()
                conn.close()

    # æ·»åŠ ç´¢å¼•
    create_index("idx_unique_int", ["date_int", "code_int"], is_unique=True)
    create_index("idx_code_int", ["code_int"])
    create_index("idx_date_int", ["date_int"])



def åŒæ­¥è¡¨ç»“æ„(conn, table_name, data_columns):
    """åŠ¨æ€æ·»åŠ ç¼ºå¤±å­—æ®µï¼ˆçº¿ç¨‹å®‰å…¨+äº‹åŠ¡é”ï¼‰"""
    try:
        cursor = conn.cursor(buffered=True)

        # 1. è·å–è¡¨çº§å†™é”
        cursor.execute(f"LOCK TABLES `{table_name}` WRITE;")

        # 2. ä½¿ç”¨ INFORMATION_SCHEMA æ£€æŸ¥å­—æ®µ
        cursor.execute(f"""
            SELECT COLUMN_NAME
            FROM INFORMATION_SCHEMA.COLUMNS
            WHERE TABLE_NAME = '{table_name}'
              AND TABLE_SCHEMA = DATABASE()
        """)
        existing_columns = {row[0] for row in cursor.fetchall()}

        # 3. è·å–é…ç½®è¡¨å­—æ®µ
        table_config = tbs.TABLE_REGISTRY.get(table_name, {})
        all_required_columns = set(table_config.get('columns', {}).keys())

        # 4. éå†å¤„ç†å­—æ®µ
        for col in data_columns:
            try:
                if col not in existing_columns:
                    if col in all_required_columns:
                        col_info = table_config['columns'][col]
                        sql_type = tbs._get_sql_type(col_info['type'])
                        alter_sql = f"ALTER TABLE `{table_name}` ADD COLUMN `{col}` {sql_type};"
                        print(f"[EXECUTE] æ‰§è¡ŒSQLï¼š{alter_sql}")
                        cursor.execute(alter_sql)
                        conn.commit()
                    else:
                        print(f"[DEBUG] å­—æ®µ {col} ä¸åœ¨é…ç½®è¡¨ä¸­ï¼Œå·²è·³è¿‡")
                else:
                    pass  # å­—æ®µå·²å­˜åœ¨ï¼Œæ— éœ€å¤„ç†
            except mysql.connector.Error as err:
                if err.errno == 1060:  # æ•è·é‡å¤å­—æ®µé”™è¯¯
                    print(f"[WARNING] å­—æ®µ {col} å¯èƒ½å·²è¢«å…¶ä»–çº¿ç¨‹åˆ›å»ºï¼Œé”™è¯¯ä¿¡æ¯ï¼š{err}")
                else:
                    raise
            except Exception as e:
                print(f"[ERROR] å¤„ç†å­—æ®µ {col} å¤±è´¥ï¼š{str(e)}")
                conn.rollback()

        # 5. é‡Šæ”¾é”
        cursor.execute("UNLOCK TABLES;")
        conn.commit()
        print(f"[SUCCESS] è¡¨ {table_name} ç»“æ„åŒæ­¥å®Œæˆ")

    except Exception as main_error:
        print(f"[CRITICAL] åŒæ­¥è¡¨ç»“æ„ä¸»æµç¨‹å¤±è´¥ï¼š{str(main_error)}")
        conn.rollback()
    finally:
        if conn.is_connected():
            cursor.close()


def sqlè¯­å¥ç”Ÿæˆå™¨(table_name, data):
    # # å‡†å¤‡æ¨¡æ¿
    # sql_template = """INSERT INTO `{table_name}` ({columns}) VALUES {values} ON DUPLICATE KEY UPDATE {update_clause};"""
    # # é¢„å¤„ç†åˆ—åå’Œæ›´æ–°å­å¥
    # columns = ', '.join([f"`{col}`" for col in data.columns])
    # update_clause = ', '.join([f"`{col}`=VALUES(`{col}`)" for col in data.columns if col not in ['date', 'code']])
    # å‡†å¤‡æ¨¡æ¿
    #
    #
    # æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦ä¸ºç©º
    if data.empty:
        print("[WARNING] è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡SQLç”Ÿæˆ")
        return ""

    # é¢„å¤„ç†æ•°æ®ï¼ˆå¦‚æ·»åŠ code_intï¼‰
    if 'code' in data.columns and 'code_int' not in data.columns:
        data.insert(0, 'code_int', data['code'].astype(int))

    sql_template = """INSERT INTO `{table_name}` ({columns})
        VALUES {values}
        ON DUPLICATE KEY UPDATE {update_clause};"""

    # é¢„å¤„ç†åˆ—å
    columns = ', '.join([f"`{col}`" for col in data.columns])

    # æŒ‡å®šå”¯ä¸€é”®åˆ—ï¼ˆè¿™äº›å­—æ®µç”¨äºæ¯”å¯¹æ•°æ®åº“ä¸­çš„è®°å½•ï¼‰
    unique_keys = ['date', 'code']

    # æ›´æ–°å­å¥ï¼šæ›´æ–°æ‰€æœ‰éå”¯ä¸€é”®çš„åˆ—
    update_clause = ', '.join(
        [f"`{col}`=VALUES(`{col}`)"
         for col in data.columns
         if col not in unique_keys]
    )
    # æ‰¹é‡å¤„ç†å€¼
    value_rows = []
    for row in data.itertuples(index=False):
        values = []
        for item in row:
            if pd.isna(item) or item in ['-', '']:
                values.append("NULL")
            elif isinstance(item, (datetime.date, datetime.datetime)):
                values.append(f"'{item.strftime('%Y-%m-%d')}'")
            elif isinstance(item, (int, float, bool)):
                values.append(str(item))
            else:
                # ä½¿ç”¨ä¸‰é‡å¼•å·é¿å…åµŒå¥—å†²çª
                cleaned_item = str(item).replace("'", "''")
                values.append(f"'{cleaned_item}'")
        value_rows.append(f"({', '.join(values)})")

    # æ‰¹é‡ç”ŸæˆSQL
    sql_statements = [
        sql_template.format(
            table_name=table_name,
            columns=columns,
            values=values,
            update_clause=update_clause
        )
        for values in value_rows
    ]

    # è¿æ¥æ‰€æœ‰è¯­å¥
    sql_txt = "\n".join(sql_statements)
    return sql_txt


def execute_raw_sql(sql, params=None, max_query_size=1024*1024, batch_size=5000):
    """æ”¹è¿›æ‰¹é‡å†™å…¥æ€§èƒ½"""
    if not sql.strip():
        print("[WARNING] SQLè¯­å¥ä¸ºç©ºï¼Œè·³è¿‡æ‰§è¡Œ")
        return False

    connection = DBManager.get_new_connection()
    if not connection:
        return False
    try:
        cursor = connection.cursor()
        statements = [s.strip() for s in sql.split(';') if s.strip()]
        if not statements:
            print("[WARNING] æ— æœ‰æ•ˆSQLè¯­å¥")
            return False

        for statement in statements:
            cursor.execute(statement)
        connection.commit()
        return True
    except Error as e:
        print(f"æ•°æ®åº“é”™è¯¯: {e}")
        connection.rollback()
        return False
    finally:
        if connection.is_connected():
            cursor.close()
            connection.close()



from itertools import islice

def batch(iterable, batch_size=100):
    iterator = iter(iterable)
    while batch := list(islice(iterator, batch_size)):
        yield batch

def check_if_first_run() -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œï¼ˆæ‰€æœ‰æŒ‡æ ‡è¡¨æ— æ•°æ®æˆ–è¡¨ä¸å­˜åœ¨ï¼‰"""
    for data_type in ['stock', 'etf', 'index']:
        table_name = INDICATOR_TABLES[data_type]
        try:
            with DBManager.get_new_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(f"SHOW TABLES LIKE '{table_name}';")
                exists = cursor.fetchone() is not None
                if exists:
                    # è¡¨å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                    query = f"SELECT 1 FROM `{table_name}` LIMIT 1"
                    result = pd.read_sql(query, conn)
                    if not result.empty:
                        return False  # å­˜åœ¨æ•°æ®ï¼Œéé¦–æ¬¡è¿è¡Œ
                else:
                    # è¡¨ä¸å­˜åœ¨ï¼Œå±äºé¦–æ¬¡è¿è¡Œ
                    return True
        except Exception as e:
            print(f"æ£€æŸ¥è¡¨ {table_name} å¤±è´¥ï¼š{str(e)}")
            return True
    return True


def sync_table_structure(table_name: str, data_columns: List[str]):
    """æ ¹æ®å®é™…æ•°æ®å­—æ®µåŠ¨æ€åŒæ­¥è¡¨ç»“æ„"""
    try:
        with DBManager.get_new_connection() as conn:
            cursor = conn.cursor()

            # 1. åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            cursor.execute(f"SHOW TABLES LIKE '{table_name}'")
            if not cursor.fetchone():
                # åŸºç¡€è¡¨ç»“æ„ï¼ˆdate, code_int, code, nameï¼‰
                create_sql = f"""
                    CREATE TABLE `{table_name}` (
                        `id` INT,
                        `date` DATE,
                        `date_int` INT,
                        `code_int` INT,
                        `code` VARCHAR(6),
                        `name` VARCHAR(20)
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;
                """
                cursor.execute(create_sql)
                print(f"åˆ›å»ºåŸºç¡€è¡¨ {table_name}")

            # 2. åŠ¨æ€æ·»åŠ æŒ‡æ ‡å­—æ®µ
            cursor.execute(f"""
                SELECT COLUMN_NAME
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_NAME = '{table_name}'
                  AND TABLE_SCHEMA = DATABASE()
            """)
            existing_columns = {row[0] for row in cursor.fetchall()}

            # 3. éå†æŒ‡æ ‡å­—æ®µï¼Œæ·»åŠ ç¼ºå¤±åˆ—
            for col in data_columns:
                if col not in existing_columns and col not in ['id', 'date','date_int', 'code_int', 'code', 'name']:
                    # è‡ªåŠ¨æ¨æ–­å­—æ®µç±»å‹ï¼ˆå‡è®¾å‡ä¸ºFLOATï¼‰
                    alter_sql = f"ALTER TABLE `{table_name}` ADD COLUMN `{col}` FLOAT;"
                    cursor.execute(alter_sql)
                    print(f"åŠ¨æ€æ·»åŠ å­—æ®µ {col} åˆ°è¡¨ {table_name}")
            conn.commit()
    except Exception as e:
        print(f"åŒæ­¥è¡¨ {table_name} ç»“æ„å¤±è´¥ï¼š{str(e)}")
        sys.exit(1)


def get_hist_data_batch(batch_codes: List[int], data_type: str) -> pd.DataFrame:
    """ä¸¥æ ¼æŒ‰æ‰¹æ¬¡æ‰§è¡Œå•æ¬¡æŸ¥è¯¢ï¼ˆæ— åˆ†å—ï¼‰ï¼Œè·å–æœ€è¿‘MAX_HISTORY_WINDOWå¤©æ•°æ®"""
    if not batch_codes:
        return pd.DataFrame()

    try:
        with DBManager.get_new_connection() as conn:
            code_list = ",".join(map(str, batch_codes))
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            date_condition = f"date >= CURDATE() - INTERVAL {MAX_HISTORY_WINDOW} DAY"
            
            query = f"""
                SELECT date, date_int, code, code_int, name, open, close, high, low, volume
                FROM {TABLE_MAP[data_type]['hist_table']}
                WHERE code_int IN ({code_list})
                  AND {date_condition}
                ORDER BY code_int, date_int ASC 
            """
            return pd.read_sql(query, conn)
    except Exception as e:
        print(f"è·å–æ‰¹æ¬¡æ•°æ®å¤±è´¥ï¼š{str(e)}")
        return pd.DataFrame()

# def get_hist_data_batch(batch_codes: List[int], data_type: str) -> pd.DataFrame:
#     """ä¸¥æ ¼æŒ‰æ‰¹æ¬¡æ‰§è¡Œå•æ¬¡æŸ¥è¯¢ï¼ˆæ— åˆ†å—ï¼‰"""
#     if not batch_codes:
#         return pd.DataFrame()

#     try:
#         with DBManager.get_new_connection() as conn:
#             code_list = ",".join(map(str, batch_codes))

#             query = f"""
#                 SELECT *
#                 FROM {TABLE_MAP[data_type]['hist_table']}
#                 WHERE code_int IN ({code_list})
#                 -- ORDER BY code_int, date DESC
#             """
#             return pd.read_sql(query, conn)
#     except Exception as e:
#         print(f"è·å–æ‰¹æ¬¡æ•°æ®å¤±è´¥ï¼š{str(e)}")
#         return pd.DataFrame()


def calculate_and_save_batch(code: int, data_type: str, batch_data: pd.DataFrame) -> pd.DataFrame:
    """ä»æ‰¹æ¬¡æ•°æ®ä¸­æå–å•ä¸ªä»£ç æ•°æ®ï¼ˆæ— æ•°æ®åº“äº¤äº’ï¼‰"""
    try:
        # ç›´æ¥ä»æ‰¹æ¬¡æ•°æ®è¿‡æ»¤
        hist_data = batch_data[batch_data['code_int'] == code].copy()
        if hist_data.empty:
            print(f"è·³è¿‡ç©ºæ•°æ®ï¼š{data_type} {code}")
            return pd.DataFrame()

        # åç»­å¤„ç†é€»è¾‘
        table_name = INDICATOR_TABLES[data_type]
        last_processed_date = get_last_processed_date(table_name, code)

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_columns = {'date', 'code', 'open', 'close', 'high', 'low', 'volume'}
        missing_columns = required_columns - set(hist_data.columns)
        if missing_columns:
            print(f"æ•°æ®ç¼ºå¤±å…³é”®åˆ— {missing_columns}ï¼Œè·³è¿‡å¤„ç†ï¼š{code}")
            return pd.DataFrame()

        # è®¡ç®—æŒ‡æ ‡
        indicators = calculate_indicators(hist_data)
        if indicators.empty:
            return pd.DataFrame()

        # è¿‡æ»¤å·²å¤„ç†æ—¥æœŸ
        if last_processed_date:
            indicators = indicators[indicators['date'] > last_processed_date]

        # è¿‡æ»¤æ— æ•ˆcci_84
        if 'cci_84' in indicators.columns:
            indicators = indicators[indicators['cci_84'] != 0]

        return indicators if not indicators.empty else pd.DataFrame()
    except Exception as e:
        print(f"å¤„ç†{data_type} {code}å¤±è´¥ï¼š{str(e)}")
        return pd.DataFrame()

def process_single_code(
    code: int,
    data_type: str,
    code_data: pd.DataFrame,
    last_processed_date: Optional[str] = None
) -> pd.DataFrame:
    """å¤„ç†å•ä¸ªä»£ç çš„è®¡ç®—é€»è¾‘ï¼ˆå®Œå…¨åŸºäºä¼ å…¥çš„code_dataï¼‰"""
    try:
        # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿï¼ˆ34æ¡ä¸ºTA-Libæœ€ä½è¦æ±‚ï¼‰
        if len(code_data) < 34:
            print(f"ä»£ç  {code} æ•°æ®ä¸è¶³34æ¡ï¼ˆå®é™…{len(code_data)}æ¡ï¼‰ï¼Œè·³è¿‡")
            return pd.DataFrame()

        # è®¡ç®—æŒ‡æ ‡
        indicators = calculate_indicators(code_data)
        if indicators.empty:
            return pd.DataFrame()

        # è¿‡æ»¤å·²å¤„ç†æ—¥æœŸ
        # ä½¿ç”¨é¢„å–çš„last_processed_dateè¿‡æ»¤æ•°æ®
        if last_processed_date:
            indicators = indicators[indicators['date'] > last_processed_date]


        # è¿‡æ»¤æ— æ•ˆcci_84
        if 'cci_84' in indicators.columns:
            indicators = indicators[indicators['cci_84'] != 0]

        return indicators if not indicators.empty else pd.DataFrame()
    except Exception as e:
        print(f"å¤„ç†ä»£ç  {code} å¤±è´¥ï¼š{str(e)}")
        return pd.DataFrame()


def main():
    start_time = time.time()
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œï¼ˆä»»ä¸€æŒ‡æ ‡è¡¨æ— æ•°æ®ï¼‰
        is_first_run = check_if_first_run()

        # é¦–æ¬¡è¿è¡Œæ—¶åŠ¨æ€åŒæ­¥è¡¨ç»“æ„
        if is_first_run:
            # å®šä¹‰æ¯ä¸ªç±»å‹çš„ç¤ºä¾‹code_int
            sample_codes = {
                'stock': 1,      # å‡è®¾code_int=1ä¸ºæœ‰æ•ˆè‚¡ç¥¨
                'etf': 159001,   # å‡è®¾code_int=159001ä¸ºæœ‰æ•ˆETF
                'index': 1       # å‡è®¾code_int=1ä¸ºæœ‰æ•ˆæŒ‡æ•°
            }

            for data_type in ['stock', 'etf', 'index']:
                table_name = INDICATOR_TABLES[data_type]
                code_int = sample_codes[data_type]

                # create_table_if_not_exists(table_name)  # ç¡®ä¿åªæ‰§è¡Œä¸€æ¬¡

                # 1. è·å–è¶³å¤Ÿçš„å†å²æ•°æ®ï¼ˆè‡³å°‘34æ¡ï¼‰
                # è·å–å†å²æ•°æ®ï¼ˆç›´æ¥ä¼ é€’æ•´æ•°ï¼‰
                hist_data = get_hist_data(code_int, data_type, last_date=None)
                if len(hist_data) < 34:
                    print(f"é”™è¯¯ï¼š{data_type}ç¤ºä¾‹æ•°æ®ä¸è¶³34æ¡ï¼ˆå½“å‰{len(hist_data)}æ¡ï¼‰ï¼Œæ— æ³•åŒæ­¥ç»“æ„ï¼")
                    sys.exit(1)

                # 2. è®¡ç®—æŒ‡æ ‡ï¼Œè·å–æ‰€æœ‰å­—æ®µ
                indicators = calculate_indicators(hist_data)
                if indicators.empty:
                    print(f"é”™è¯¯ï¼š{data_type}æŒ‡æ ‡è®¡ç®—å¤±è´¥ï¼")
                    sys.exit(1)

                # 3. åŠ¨æ€åŒæ­¥è¡¨ç»“æ„ï¼ˆåŸºäºå®é™…å­—æ®µï¼‰
                sync_table_structure(table_name, indicators.columns)

            print("é¦–æ¬¡è¿è¡Œè¡¨ç»“æ„åŒæ­¥å®Œæˆ")




        batch_size = 500
        max_workers = 10

        with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # with ThreadPoolExecutor(max_workers=max_workers) as executor:
            for data_type in ['etf', 'index', 'stock']:
                codes = get_latest_codes(data_type)
                print(f"å¼€å§‹å¤„ç† {data_type} å…± {len(codes)} ä¸ªä»£ç ")

                for batch_idx in range(0, len(codes), batch_size):
                    batch_codes = codes[batch_idx:batch_idx + batch_size]
                    print(f"å¤„ç†æ‰¹æ¬¡ {batch_idx//batch_size+1}ï¼Œä»£ç æ•°ï¼š{len(batch_codes)}")

                    # 1. è·å–æœ¬æ‰¹æ¬¡å†å²æ•°æ®
                    batch_data = get_hist_data_batch(batch_codes, data_type)
                    if batch_data.empty:
                        print(f"æ‰¹æ¬¡ {batch_idx//batch_size+1} æ— æ•°æ®ï¼Œè·³è¿‡")
                        continue

                    # 2. æ‰¹é‡è·å–æœ€åå¤„ç†æ—¥æœŸï¼ˆå…³é”®ä¿®æ”¹ç‚¹ï¼‰
                    # ä»æœ¬æ‰¹æ•°æ®ä¸­æå–æ‰€æœ‰å”¯ä¸€ä»£ç 
                    unique_codes_in_batch = batch_data['code_int'].unique().tolist()
                    last_dates_map = get_last_processed_dates_batch(
                        INDICATOR_TABLES[data_type],
                        unique_codes_in_batch
                    )

                    # 3. å¹¶è¡Œå¤„ç†æœ¬æ‰¹æ¬¡ä»£ç 
                    futures = []
                    for code in batch_codes:
                        # ä»æ‰¹æ¬¡æ•°æ®ä¸­æå–å•ä¸ªä»£ç æ•°æ®
                        code_data = batch_data[batch_data['code_int'] == code].copy()
                        if code_data.empty:
                            continue
                        # æäº¤ä»»åŠ¡æ—¶ä¼ å…¥é¢„å–çš„æœ€åå¤„ç†æ—¥æœŸ
                        futures.append(executor.submit(
                            process_single_code,
                            code=code,
                            data_type=data_type,
                            code_data=code_data,
                            last_processed_date=last_dates_map.get(code, None)
                        ))

                    # 4. åˆå¹¶å¹¶æäº¤æœ¬æ‰¹æ¬¡ç»“æœ
                    valid_dfs = []
                    for future in as_completed(futures):
                        df = future.result()
                        if df is not None and not df.empty:
                            valid_dfs.append(df)

                    if valid_dfs:
                        combined_data = pd.concat(valid_dfs, ignore_index=True)
                        sync_and_save(INDICATOR_TABLES[data_type], combined_data)
                        print(f"æ‰¹æ¬¡æäº¤æˆåŠŸï¼Œè®°å½•æ•°ï¼š{len(combined_data)}")
                    else:
                        print(f"æœ¬æ‰¹æ¬¡æ— æœ‰æ•ˆæ•°æ®")
    finally:
        print(f"\nğŸ•’ æ€»è€—æ—¶: {time.time()-start_time:.2f}ç§’")  # ç¡®ä¿å¼‚å¸¸æ—¶ä¹Ÿè¾“å‡º

if __name__ == "__main__":
    main()

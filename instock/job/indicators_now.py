#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# 
''' 
2025å¹´6æœˆ21æ—¥ 
1ã€åªè®¡ç®—KDJï¼ŒBOLLï¼ŒWRï¼ŒCCIï¼ŒOBVæ•°æ®ï¼›
2ã€å»é™¤CCI84ä¸ºç©ºçš„æ•°æ®(è¯¥è®¡ç®—å€¼è‡³å°‘éœ€è¦84ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®)ï¼›
3ã€è®¡ç®—çª—å£å€¼ MAX_HISTORY_WINDOW æ”¹ä¸º 60

'''

# åœ¨é¡¹ç›®è¿è¡Œæ—¶ï¼Œä¸´æ—¶å°†é¡¹ç›®è·¯å¾„æ·»åŠ åˆ°ç¯å¢ƒå˜é‡
import os.path
import sys
cpath_current = os.path.dirname(os.path.dirname(__file__))
cpath = os.path.abspath(os.path.join(cpath_current, os.pardir))
sys.path.append(cpath)

import pandas as pd
import numpy as np
# æ›¿æ¢ import talib as tl
import ta
import mysql.connector
import datetime
import threading
from typing import Optional  # æ–°å¢å¯¼å…¥
# from datetime import datetime, timedelta
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple
from mysql.connector import Error
# from instock.lib.database import DBManager
import sqlalchemy
import time
import instock.core.tablestructure as tbs
from instock.lib.database import db_host, db_user, db_password, db_database, db_charset



def calculate_indicators(data):
    # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦æ»¡è¶³æœ€å°çª—å£ï¼ˆä¾‹å¦‚MACDéœ€è¦è‡³å°‘34æ¡æ•°æ®ï¼‰
    min_window = 34  # æ ¹æ®TA-LibæŒ‡æ ‡è¦æ±‚è°ƒæ•´
    if len(data) < min_window:
        print(f"[WARNING] æ•°æ®ä¸è¶³{min_window}æ¡ï¼Œæ— æ³•è®¡ç®—æŒ‡æ ‡")
        return pd.DataFrame()

    daily_data_indicators = pd.DataFrame()
    data = data.sort_values(by='date', ascending=True)
   # åŸå§‹å­—æ®µ
    daily_data_indicators['date'] = data['date']
    daily_data_indicators['code'] = data['code']


    # æ ¼å¼åŒ–æ—¥æœŸä¸º YYYYMMDD
    daily_data_indicators['date_int'] = data['date'].astype(str).str.replace('-', '')

    if 'code_int' in data.columns:
        daily_data_indicators['code_int'] = data['code_int']
    if 'name' in data.columns:
        daily_data_indicators['name'] = data['name']
    daily_data_indicators['close'] = data['close']
    '''
    è®¡ç®—ETFæ•°æ®çš„å„ç§æŒ‡æ ‡ã€‚

    å‚æ•°:
    data (pd.DataFrame): åŒ…å«ETFæ¯æ—¥æ•°æ®çš„DataFrameï¼Œè‡³å°‘åŒ…å«'open' ,'close', 'high', 'low', 'volume'åˆ— æ”¶ç›˜ä»·ï¼Œæœ€é«˜ä»·ï¼Œæœ€ä½ä»·ï¼Œæˆäº¤é‡

    è¿”å›:
    pd.DataFrame: åŒ…å«è®¡ç®—åæŒ‡æ ‡æ•°æ®çš„DataFrame
    '''


    # è®¡ç®—KDJçš„Kå’ŒD
    # daily_data_indicators['kdjk'], daily_data_indicators['kdjd'] = tl.STOCH(
    #     data['high'],
    #     data['low'],
    #     data['close'],
    #     fastk_period=9,    # é»˜è®¤å‚æ•°éœ€æ˜¾å¼æŒ‡å®š
    #     slowk_period=5,
    #     slowk_matype=1,
    #     slowd_period=5,
    #     slowd_matype=1
    # )

    # ä½¿ç”¨taåº“è®¡ç®—æŒ‡æ ‡
    # 1. è®¡ç®—MACD
    macd = ta.trend.MACD(close=data['close'])
    daily_data_indicators['macd'] = macd.macd()
    daily_data_indicators['macds'] = macd.macd_signal()
    daily_data_indicators['macdh'] = macd.macd_diff()
    
    # 2. è®¡ç®—KDJ
    stoch = ta.momentum.StochasticOscillator(
        high=data['high'],
        low=data['low'],
        close=data['close'],
        window=9,
        smooth_window=5
    )
    daily_data_indicators['kdjk'] = stoch.stoch()
    daily_data_indicators['kdjd'] = stoch.stoch_signal()
    daily_data_indicators['kdjj'] = 3 * daily_data_indicators['kdjk'] - 2 * daily_data_indicators['kdjd']
    
    # 3. è®¡ç®—BOLL
    boll = ta.volatility.BollingerBands(close=data['close'], window=20, window_dev=2)
    daily_data_indicators['boll_ub'] = boll.bollinger_hband()
    daily_data_indicators['boll'] = boll.bollinger_mavg()
    daily_data_indicators['boll_lb'] = boll.bollinger_lband()
    
    # 4. è®¡ç®—WR
    daily_data_indicators['wr_6'] = ta.momentum.WilliamsRIndicator(
        high=data['high'], low=data['low'], close=data['close'], lbp=6
    ).williams_r()
    
    daily_data_indicators['wr_10'] = ta.momentum.WilliamsRIndicator(
        high=data['high'], low=data['low'], close=data['close'], lbp=10
    ).williams_r()
    
    daily_data_indicators['wr_14'] = ta.momentum.WilliamsRIndicator(
        high=data['high'], low=data['low'], close=data['close'], lbp=14
    ).williams_r()
    
    # 5. è®¡ç®—CCI
    daily_data_indicators['cci'] = ta.trend.CCIIndicator(
        high=data['high'], low=data['low'], close=data['close'], window=20
    ).cci()
    
    # 6. è®¡ç®—OBV
    daily_data_indicators['obv'] = ta.volume.OnBalanceVolumeIndicator(
        close=data['close'], volume=data['volume']
    ).on_balance_volume()
    
    return daily_data_indicators




# è¡¨æ˜ å°„é…ç½®
TABLE_MAP = {
    'stock': {
        'hist_table': 'cn_stock_hist_daily',
        'info_table': 'cn_stock_info'
    },
    'etf': {
        'hist_table': 'cn_etf_hist_daily',
        'info_table': 'cn_etf_info'
    },
    'index': {
        'hist_table': 'cn_index_hist_daily',
        'info_table': 'cn_index_info'
    }
}

INDICATOR_TABLES = {
    'stock': 'cn_stock_indicators',
    'etf': 'cn_etf_indicators',
    'index': 'cn_index_indicators'
}

# æ•°æ®åº“è¿æ¥é…ç½®
MAX_HISTORY_WINDOW = 60  # æŒ‡æ ‡è®¡ç®—æ‰€éœ€æœ€å¤§å†å²çª—å£


def get_latest_codes(data_type: str) -> List[str]:
    """è·å–æŒ‡å®šç±»å‹çš„æœ€æ–°ä»£ç åˆ—è¡¨"""
    try:
        with DBManager.get_new_connection() as conn:
            query = f"""
                SELECT code_int
                FROM {TABLE_MAP[data_type]['info_table']}
                WHERE date = (SELECT MAX(date) FROM {TABLE_MAP[data_type]['info_table']})
            """
            return pd.read_sql(query, conn)['code_int'].tolist()
    except Exception as e:
        print(f"è·å–{data_type}ä»£ç å¤±è´¥ï¼š{str(e)}")
        return []

def get_hist_data(code: int, data_type: str, last_date: str = None) -> pd.DataFrame:
    """è·å–å¸¦æ—¥æœŸèŒƒå›´çš„è¡Œæƒ…æ•°æ®"""
    try:
        with DBManager.get_new_connection() as conn:
            base_query = f"""
                SELECT * FROM {TABLE_MAP[data_type]['hist_table']}
                WHERE code_int = '{code}'
            """

            if last_date:
                query = f"""
                    {base_query}
                    AND date >= (
                        SELECT DATE_SUB('{last_date}', INTERVAL {MAX_HISTORY_WINDOW} DAY)
                        FROM DUAL
                    )
                """
            else:
                query = base_query + " ORDER BY date ASC LIMIT 1000"

            data = pd.read_sql(query, conn)
            # --- è°ƒè¯•7: éªŒè¯åŸå§‹æ•°æ®è´¨é‡ ---
            # print(f"[DEBUG] {code} åŸå§‹æ•°æ®ç»Ÿè®¡:")
            # print("è®°å½•æ•°:", len(data))
            # print("æ—¶é—´èŒƒå›´:", data['date'].min(), "è‡³", data['date'].max())
            # print("ç¼ºå¤±å€¼ç»Ÿè®¡:")
            # print(data[['close', 'high', 'low', 'volume']].isnull().sum())

            return data.sort_values('date', ascending=True) if not data.empty else pd.DataFrame()
            # return data.sort_values('date', ascending=True)
    except Exception as e:
        print(f"è·å–{data_type}å†å²æ•°æ®å¤±è´¥ï¼š{code_int}-{str(e)}")
        return pd.DataFrame()

def calculate_and_save(code: str, data_type: str):
    """å®Œæ•´çš„å¤„ç†æµæ°´çº¿"""
    try:
        # æ£€æŸ¥ç›®æ ‡è¡¨æ˜¯å¦å­˜åœ¨
        table_name = INDICATOR_TABLES[data_type]
        # create_table_if_not_exists(table_name)

        # è·å–æœ€æ–°å¤„ç†æ—¥æœŸ
        last_processed_date = get_last_processed_date(table_name, code)

        # è·å–å†å²æ•°æ®ï¼ˆå¢é‡é€»è¾‘ï¼‰
        hist_data = get_hist_data(code, data_type, last_processed_date)
        if hist_data.empty:
            print(f"è·³è¿‡ç©ºæ•°æ®ï¼š{data_type} {code}")
            return

        # æ£€æŸ¥å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨
        required_columns = {'date', 'code','open', 'close', 'high', 'low', 'volume'}
        missing_columns = required_columns - set(hist_data.columns)
        if missing_columns:
            print(f"æ•°æ®ç¼ºå¤±å…³é”®åˆ— {missing_columns}ï¼Œè·³è¿‡å¤„ç†ï¼š{code}")
            return

        # è®¡ç®—æŒ‡æ ‡
        # print(f"\n=== å¼€å§‹å¤„ç† {code} ===")
        indicators = calculate_indicators(hist_data)

        # --- è°ƒè¯•5: è¾“å‡ºå‰5è¡Œæ•°æ®æ ·æœ¬ ---
        # print(f"[DEBUG] {code} è®¡ç®—ç»“æœæ ·æœ¬:")
        # print(indicators.head())

        # --- è°ƒè¯•6: æ£€æŸ¥æ˜¯å¦å­˜åœ¨è´Ÿæ— ç©·æˆ–é›¶å€¼ ---
        # print(f"[DEBUG] {code} å¼‚å¸¸å€¼ç»Ÿè®¡:")
        # print("Inf values:", (indicators == np.inf).sum().sum())
        # print("-Inf values:", (indicators == -np.inf).sum().sum())
        # print("Zero values:", (indicators == 0).sum().sum())

        # è¿‡æ»¤å·²å­˜åœ¨æ•°æ®
        if last_processed_date:
            indicators = indicators[indicators['date'] >= last_processed_date]

        # å†™å…¥æ•°æ®åº“å‰æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
        # if not indicators.empty:
        #     # æ–°å¢è¿‡æ»¤æ¡ä»¶ï¼šåˆ é™¤ cci_84 ä¸º0çš„è¡Œ
        #     if 'cci_84' in indicators.columns:
        #         indicators = indicators[indicators['cci_84'] != 0]
        #     if not indicators.empty:
        #         sync_and_save(table_name, indicators)
        #         print(f"æ›´æ–°{data_type}æŒ‡æ ‡ï¼š{code} {len(indicators)}æ¡")


        if not indicators.empty:
            sync_and_save(table_name, indicators)
            print(f"æ›´æ–°{data_type}æŒ‡æ ‡ï¼š{code} {len(indicators)}æ¡")
        else:
            print(f"æ— æ–°æ•°æ®éœ€æ›´æ–°ï¼š{code}")
    except Exception as e:
        print(f"å¤„ç†{data_type} {code}å¤±è´¥ï¼š{str(e)}")

def get_latest_codes(data_type: str) -> List[int]:
    """è·å–æŒ‡å®šç±»å‹çš„æœ€æ–°ä»£ç åˆ—è¡¨ï¼ˆè¿”å›æ•´æ•°åˆ—è¡¨ï¼‰"""
    try:
        with DBManager.get_new_connection() as conn:
            query = f"""
                SELECT code_int
                FROM {TABLE_MAP[data_type]['info_table']}
                WHERE date = (SELECT MAX(date) FROM {TABLE_MAP[data_type]['info_table']})
            """
            df = pd.read_sql(query, conn)
            return df['code_int'].astype(int).tolist()  # å¼ºåˆ¶è½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨
    except Exception as e:
        print(f"è·å–{data_type}ä»£ç å¤±è´¥ï¼š{str(e)}")
        return []

def get_last_processed_date(table: str, code: int) -> str:
    """è·å–æŒ‡å®šä»£ç çš„æœ€åå¤„ç†æ—¥æœŸ"""
    try:
        with DBManager.get_new_connection() as conn:
            query = f"""
                SELECT MAX(date) AS last_date
                FROM {table}
                WHERE code_int = '{code}'
            """
            result = pd.read_sql(query, conn)
            return result.iloc[0]['last_date']
    except:
        return None


def get_last_processed_dates_batch(table: str, codes: List[int]) -> Dict[int, str]:
    """æ‰¹é‡è·å–å¤šä¸ªä»£ç çš„æœ€åå¤„ç†æ—¥æœŸ"""
    if not codes:
        return {}

    try:
        with DBManager.get_new_connection() as conn:
            code_list = ",".join(map(str, codes))
            query = f"""
                SELECT code_int, MAX(date) AS last_date
                FROM {table}
                WHERE code_int IN ({code_list})
                GROUP BY code_int
            """
            df = pd.read_sql(query, conn)
            return df.set_index('code_int')['last_date'].to_dict()
    except Exception as e:
        print(f"è·å–æœ€åå¤„ç†æ—¥æœŸå¤±è´¥ï¼š{str(e)}")
        return {}


def sync_and_save(table_name: str, data: pd.DataFrame):
    sql_txt = sqlè¯­å¥ç”Ÿæˆå™¨(table_name, data)
    execute_raw_sql(sql_txt)


class DBManager:
    @staticmethod
    def get_new_connection():
        """åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªæ–°çš„æ•°æ®åº“è¿æ¥"""
        try:
            connection = mysql.connector.connect(
                host=db_host,
                user=db_user,
                password=db_password,
                database=db_database,
                charset=db_charset
            )
            return connection
        except Error as e:
            print(f"Error while connecting to MySQL: {e}")
            return None

    @staticmethod
    def execute_sql(sql: str, params=None):
        """å®‰å…¨æ‰§è¡Œ SQL è¯­å¥"""
        connection = DBManager.get_new_connection()
        if connection:
            try:
                cursor = connection.cursor(buffered=True)
                cursor.execute(sql, params)
                connection.commit()
                cursor.close()
            except Error as e:
                print(f"Error while executing SQL: {e}")
            finally:
                if connection.is_connected():
                    connection.close()


def create_table_if_not_exists(table_name):
    # åˆ›å»ºè¡¨ï¼ˆä¸å«ç´¢å¼•ï¼‰
    create_table_sql = f"""
        CREATE TABLE IF NOT EXISTS `{table_name}` (
            `id` INT AUTO_INCREMENT PRIMARY KEY,
            `date` DATE,
            `date_int` INT,
            `code_int` INT,
            `code` VARCHAR(6),
            `name` VARCHAR(20)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;
    """
    DBManager.execute_sql(create_table_sql)

    # æ£€æŸ¥å¹¶æ·»åŠ idåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    conn = DBManager.get_new_connection()
    if conn:
        try:
            cursor = conn.cursor()
            # æ£€æŸ¥idåˆ—æ˜¯å¦å­˜åœ¨
            check_sql = f"""
                SELECT COUNT(*)
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_SCHEMA = DATABASE()
                  AND TABLE_NAME = '{table_name}'
                  AND COLUMN_NAME = 'id';
            """
            cursor.execute(check_sql)
            count = cursor.fetchone()[0]
            if count == 0:
                # æ·»åŠ idåˆ—
                alter_sql = f"""
                    ALTER TABLE `{table_name}`
                    ADD COLUMN `id` INT AUTO_INCREMENT PRIMARY KEY FIRST;
                """
                cursor.execute(alter_sql)
                conn.commit()
                print(f"è¡¨ {table_name} æˆåŠŸæ·»åŠ  id åˆ—")
        except Error as e:
            print(f"æ£€æŸ¥æˆ–æ·»åŠ  id åˆ—å¤±è´¥: {e}")
            conn.rollback()
        finally:
            if conn.is_connected():
                cursor.close()
                conn.close()

    # åˆ›å»ºç´¢å¼•ï¼ˆä¿®å¤ Unread result found é—®é¢˜ï¼‰
    def create_index(index_name, columns, is_unique=False):
        conn = DBManager.get_new_connection()
        try:
            cursor = conn.cursor()
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            check_sql = f"""
                SELECT COUNT(*)
                FROM information_schema.STATISTICS
                WHERE table_name = '{table_name}'
                  AND index_name = '{index_name}';
            """
            cursor.execute(check_sql)
            result = cursor.fetchall()  # å¼ºåˆ¶è¯»å–ç»“æœ
            if result[0][0] == 0:
                index_type = "UNIQUE" if is_unique else ""
                create_sql = f"""
                    CREATE {index_type} INDEX `{index_name}`
                    ON `{table_name}` ({', '.join(columns)});
                """
                cursor.execute(create_sql)
                conn.commit()
        except Error as e:
            print(f"åˆ›å»ºç´¢å¼• {index_name} å¤±è´¥: {e}")
            conn.rollback()
        finally:
            if conn.is_connected():
                cursor.close()
                conn.close()

    # æ·»åŠ ç´¢å¼•
    create_index("idx_unique_int", ["date_int", "code_int"], is_unique=True)
    create_index("idx_code_int", ["code_int"])
    create_index("idx_date_int", ["date_int"])






def sqlè¯­å¥ç”Ÿæˆå™¨(table_name, data):
    # # å‡†å¤‡æ¨¡æ¿
    # sql_template = """INSERT INTO `{table_name}` ({columns}) VALUES {values} ON DUPLICATE KEY UPDATE {update_clause};"""
    # # é¢„å¤„ç†åˆ—åå’Œæ›´æ–°å­å¥
    # columns = ', '.join([f"`{col}`" for col in data.columns])
    # update_clause = ', '.join([f"`{col}`=VALUES(`{col}`)" for col in data.columns if col not in ['date', 'code']])
    # å‡†å¤‡æ¨¡æ¿
    #
    #
    # æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦ä¸ºç©º
    if data.empty:
        print("[WARNING] è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡SQLç”Ÿæˆ")
        return ""

    # é¢„å¤„ç†æ•°æ®ï¼ˆå¦‚æ·»åŠ code_intï¼‰
    if 'code' in data.columns and 'code_int' not in data.columns:
        data.insert(0, 'code_int', data['code'].astype(int))

    sql_template = """INSERT INTO `{table_name}` ({columns})
        VALUES {values}
        ON DUPLICATE KEY UPDATE {update_clause};"""

    # é¢„å¤„ç†åˆ—å
    columns = ', '.join([f"`{col}`" for col in data.columns])

    # æŒ‡å®šå”¯ä¸€é”®åˆ—ï¼ˆè¿™äº›å­—æ®µç”¨äºæ¯”å¯¹æ•°æ®åº“ä¸­çš„è®°å½•ï¼‰
    unique_keys = ['date', 'code']

    # æ›´æ–°å­å¥ï¼šæ›´æ–°æ‰€æœ‰éå”¯ä¸€é”®çš„åˆ—
    update_clause = ', '.join(
        [f"`{col}`=VALUES(`{col}`)"
         for col in data.columns
         if col not in unique_keys]
    )
    # æ‰¹é‡å¤„ç†å€¼
    value_rows = []
    for row in data.itertuples(index=False):
        values = []
        for item in row:
            if pd.isna(item) or item in ['-', '']:
                values.append("NULL")
            elif isinstance(item, (datetime.date, datetime.datetime)):
                values.append(f"'{item.strftime('%Y-%m-%d')}'")
            elif isinstance(item, (int, float, bool)):
                values.append(str(item))
            else:
                # ä½¿ç”¨ä¸‰é‡å¼•å·é¿å…åµŒå¥—å†²çª
                cleaned_item = str(item).replace("'", "''")
                values.append(f"'{cleaned_item}'")
        value_rows.append(f"({', '.join(values)})")

    # æ‰¹é‡ç”ŸæˆSQL
    sql_statements = [
        sql_template.format(
            table_name=table_name,
            columns=columns,
            values=values,
            update_clause=update_clause
        )
        for values in value_rows
    ]

    # è¿æ¥æ‰€æœ‰è¯­å¥
    sql_txt = "\n".join(sql_statements)
    return sql_txt


def execute_raw_sql(sql, params=None, max_query_size=1024*1024, batch_size=5000):
    """æ”¹è¿›æ‰¹é‡å†™å…¥æ€§èƒ½"""
    if not sql.strip():
        print("[WARNING] SQLè¯­å¥ä¸ºç©ºï¼Œè·³è¿‡æ‰§è¡Œ")
        return False

    connection = DBManager.get_new_connection()
    if not connection:
        return False
    try:
        cursor = connection.cursor()
        statements = [s.strip() for s in sql.split(';') if s.strip()]
        if not statements:
            print("[WARNING] æ— æœ‰æ•ˆSQLè¯­å¥")
            return False

        for statement in statements:
            cursor.execute(statement)
        connection.commit()
        return True
    except Error as e:
        print(f"æ•°æ®åº“é”™è¯¯: {e}")
        connection.rollback()
        return False
    finally:
        if connection.is_connected():
            cursor.close()
            connection.close()



from itertools import islice

def batch(iterable, batch_size=100):
    iterator = iter(iterable)
    while batch := list(islice(iterator, batch_size)):
        yield batch

def check_if_first_run() -> bool:
    """æ£€æŸ¥æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œï¼ˆæ‰€æœ‰æŒ‡æ ‡è¡¨æ— æ•°æ®æˆ–è¡¨ä¸å­˜åœ¨ï¼‰"""
    for data_type in ['stock', 'etf', 'index']:
        table_name = INDICATOR_TABLES[data_type]
        try:
            with DBManager.get_new_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(f"SHOW TABLES LIKE '{table_name}';")
                exists = cursor.fetchone() is not None
                if exists:
                    # è¡¨å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                    query = f"SELECT 1 FROM `{table_name}` LIMIT 1"
                    result = pd.read_sql(query, conn)
                    if not result.empty:
                        return False  # å­˜åœ¨æ•°æ®ï¼Œéé¦–æ¬¡è¿è¡Œ
                else:
                    # è¡¨ä¸å­˜åœ¨ï¼Œå±äºé¦–æ¬¡è¿è¡Œ
                    return True
        except Exception as e:
            print(f"æ£€æŸ¥è¡¨ {table_name} å¤±è´¥ï¼š{str(e)}")
            return True
    return True


def sync_table_structure(table_name: str, data_columns: List[str]):
    """æ ¹æ®å®é™…æ•°æ®å­—æ®µåŠ¨æ€åŒæ­¥è¡¨ç»“æ„"""
    try:
        with DBManager.get_new_connection() as conn:
            cursor = conn.cursor()

            # 1. åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            cursor.execute(f"SHOW TABLES LIKE '{table_name}'")
            if not cursor.fetchone():
                # åŸºç¡€è¡¨ç»“æ„ï¼ˆdate, code_int, code, nameï¼‰
                create_sql = f"""
                    CREATE TABLE `{table_name}` (
                        `id` INT AUTO_INCREMENT PRIMARY KEY,
                        `date` DATE,
                        `date_int` INT,
                        `code_int` INT,
                        `code` VARCHAR(6),
                        `name` VARCHAR(20),
                        INDEX `idx_date_int` (`date_int`),
                        INDEX `idx_code_int` (`code_int`),
                        INDEX `idx_unique_int` (`code_int`,`date_int`)
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;
                """
                cursor.execute(create_sql)
                print(f"åˆ›å»ºåŸºç¡€è¡¨ {table_name}")

            # 2. åŠ¨æ€æ·»åŠ æŒ‡æ ‡å­—æ®µ
            cursor.execute(f"""
                SELECT COLUMN_NAME
                FROM INFORMATION_SCHEMA.COLUMNS
                WHERE TABLE_NAME = '{table_name}'
                  AND TABLE_SCHEMA = DATABASE()
            """)
            existing_columns = {row[0] for row in cursor.fetchall()}

            # 3. éå†æŒ‡æ ‡å­—æ®µï¼Œæ·»åŠ ç¼ºå¤±åˆ—
            for col in data_columns:
                if col not in existing_columns and col not in ['id', 'date','date_int', 'code_int', 'code', 'name']:
                    # è‡ªåŠ¨æ¨æ–­å­—æ®µç±»å‹ï¼ˆå‡è®¾å‡ä¸ºFLOATï¼‰
                    alter_sql = f"ALTER TABLE `{table_name}` ADD COLUMN `{col}` FLOAT;"
                    cursor.execute(alter_sql)
                    print(f"åŠ¨æ€æ·»åŠ å­—æ®µ {col} åˆ°è¡¨ {table_name}")
            conn.commit()
    except Exception as e:
        print(f"åŒæ­¥è¡¨ {table_name} ç»“æ„å¤±è´¥ï¼š{str(e)}")
        sys.exit(1)


def get_hist_data_batch(batch_codes: List[int], data_type: str) -> pd.DataFrame:
    """ä¸¥æ ¼æŒ‰æ‰¹æ¬¡æ‰§è¡Œå•æ¬¡æŸ¥è¯¢ï¼ˆæ— åˆ†å—ï¼‰ï¼Œè·å–æœ€è¿‘MAX_HISTORY_WINDOWå¤©æ•°æ®"""
    if not batch_codes:
        return pd.DataFrame()

    try:
        with DBManager.get_new_connection() as conn:
            code_list = ",".join(map(str, batch_codes))
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            date_condition = f"date >= CURDATE() - INTERVAL {MAX_HISTORY_WINDOW} DAY"
            
            query = f"""
                SELECT date, date_int, code, code_int, name, open, close, high, low, volume
                FROM {TABLE_MAP[data_type]['hist_table']}
                WHERE code_int IN ({code_list})
                  AND {date_condition}
                ORDER BY code_int, date_int ASC 
            """
            return pd.read_sql(query, conn)
    except Exception as e:
        print(f"è·å–æ‰¹æ¬¡æ•°æ®å¤±è´¥ï¼š{str(e)}")
        return pd.DataFrame()

# def get_hist_data_batch(batch_codes: List[int], data_type: str) -> pd.DataFrame:
#     """ä¸¥æ ¼æŒ‰æ‰¹æ¬¡æ‰§è¡Œå•æ¬¡æŸ¥è¯¢ï¼ˆæ— åˆ†å—ï¼‰"""
#     if not batch_codes:
#         return pd.DataFrame()

#     try:
#         with DBManager.get_new_connection() as conn:
#             code_list = ",".join(map(str, batch_codes))

#             query = f"""
#                 SELECT *
#                 FROM {TABLE_MAP[data_type]['hist_table']}
#                 WHERE code_int IN ({code_list})
#                 -- ORDER BY code_int, date DESC
#             """
#             return pd.read_sql(query, conn)
#     except Exception as e:
#         print(f"è·å–æ‰¹æ¬¡æ•°æ®å¤±è´¥ï¼š{str(e)}")
#         return pd.DataFrame()


def calculate_and_save_batch(code: int, data_type: str, batch_data: pd.DataFrame) -> pd.DataFrame:
    """ä»æ‰¹æ¬¡æ•°æ®ä¸­æå–å•ä¸ªä»£ç æ•°æ®ï¼ˆæ— æ•°æ®åº“äº¤äº’ï¼‰"""
    try:
        # ç›´æ¥ä»æ‰¹æ¬¡æ•°æ®è¿‡æ»¤
        hist_data = batch_data[batch_data['code_int'] == code].copy()
        if hist_data.empty:
            print(f"è·³è¿‡ç©ºæ•°æ®ï¼š{data_type} {code}")
            return pd.DataFrame()

        # åç»­å¤„ç†é€»è¾‘
        table_name = INDICATOR_TABLES[data_type]
        last_processed_date = get_last_processed_date(table_name, code)

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_columns = {'date', 'code', 'open', 'close', 'high', 'low', 'volume'}
        missing_columns = required_columns - set(hist_data.columns)
        if missing_columns:
            print(f"æ•°æ®ç¼ºå¤±å…³é”®åˆ— {missing_columns}ï¼Œè·³è¿‡å¤„ç†ï¼š{code}")
            return pd.DataFrame()

        # è®¡ç®—æŒ‡æ ‡
        indicators = calculate_indicators(hist_data)
        if indicators.empty:
            return pd.DataFrame()

        # è¿‡æ»¤å·²å¤„ç†æ—¥æœŸ
        if last_processed_date:
            indicators = indicators[indicators['date'] >= last_processed_date]

        # è¿‡æ»¤æ— æ•ˆcci_84
        # if 'cci_84' in indicators.columns:
        #     indicators = indicators[indicators['cci_84'] != 0]

        return indicators if not indicators.empty else pd.DataFrame()
    except Exception as e:
        print(f"å¤„ç†{data_type} {code}å¤±è´¥ï¼š{str(e)}")
        return pd.DataFrame()

def process_single_code(
    code: int,
    data_type: str,
    code_data: pd.DataFrame,
    last_processed_date: Optional[str] = None
) -> pd.DataFrame:
    """å¤„ç†å•ä¸ªä»£ç çš„è®¡ç®—é€»è¾‘ï¼ˆå®Œå…¨åŸºäºä¼ å…¥çš„code_dataï¼‰"""
    try:
        # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦è¶³å¤Ÿï¼ˆ34æ¡ä¸ºTA-Libæœ€ä½è¦æ±‚ï¼‰
        if len(code_data) < 34:
            print(f"ä»£ç  {code} æ•°æ®ä¸è¶³34æ¡ï¼ˆå®é™…{len(code_data)}æ¡ï¼‰ï¼Œè·³è¿‡")
            return pd.DataFrame()

        # è®¡ç®—æŒ‡æ ‡
        indicators = calculate_indicators(code_data)
        if indicators.empty:
            return pd.DataFrame()

        # è¿‡æ»¤å·²å¤„ç†æ—¥æœŸ
        # ä½¿ç”¨é¢„å–çš„last_processed_dateè¿‡æ»¤æ•°æ®
        if last_processed_date:
            indicators = indicators[indicators['date'] >= last_processed_date]


        # è¿‡æ»¤æ— æ•ˆcci_84
        # if 'cci_84' in indicators.columns:
        #     indicators = indicators[indicators['cci_84'] != 0]

        return indicators if not indicators.empty else pd.DataFrame()
    except Exception as e:
        print(f"å¤„ç†ä»£ç  {code} å¤±è´¥ï¼š{str(e)}")
        return pd.DataFrame()


def main():
    start_time = time.time()
    try:
        '''
        # æ£€æŸ¥æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œï¼ˆä»»ä¸€æŒ‡æ ‡è¡¨æ— æ•°æ®ï¼‰
        is_first_run = check_if_first_run()

        # é¦–æ¬¡è¿è¡Œæ—¶åŠ¨æ€åŒæ­¥è¡¨ç»“æ„
        if is_first_run:
            # å®šä¹‰æ¯ä¸ªç±»å‹çš„ç¤ºä¾‹code_int
            sample_codes = {
                'stock': 1,      # å‡è®¾code_int=1ä¸ºæœ‰æ•ˆè‚¡ç¥¨
                'etf': 159001,   # å‡è®¾code_int=159001ä¸ºæœ‰æ•ˆETF
                'index': 1       # å‡è®¾code_int=1ä¸ºæœ‰æ•ˆæŒ‡æ•°
            }

            for data_type in ['stock', 'etf', 'index']:
                table_name = INDICATOR_TABLES[data_type]
                code_int = sample_codes[data_type]

                # create_table_if_not_exists(table_name)  # ç¡®ä¿åªæ‰§è¡Œä¸€æ¬¡

                # 1. è·å–è¶³å¤Ÿçš„å†å²æ•°æ®ï¼ˆè‡³å°‘34æ¡ï¼‰
                # è·å–å†å²æ•°æ®ï¼ˆç›´æ¥ä¼ é€’æ•´æ•°ï¼‰
                hist_data = get_hist_data(code_int, data_type, last_date=None)
                if len(hist_data) < 34:
                    print(f"é”™è¯¯ï¼š{data_type}ç¤ºä¾‹æ•°æ®ä¸è¶³34æ¡ï¼ˆå½“å‰{len(hist_data)}æ¡ï¼‰ï¼Œæ— æ³•åŒæ­¥ç»“æ„ï¼")
                    sys.exit(1)

                # 2. è®¡ç®—æŒ‡æ ‡ï¼Œè·å–æ‰€æœ‰å­—æ®µ
                indicators = calculate_indicators(hist_data)
                if indicators.empty:
                    print(f"é”™è¯¯ï¼š{data_type}æŒ‡æ ‡è®¡ç®—å¤±è´¥ï¼")
                    sys.exit(1)

                # 3. åŠ¨æ€åŒæ­¥è¡¨ç»“æ„ï¼ˆåŸºäºå®é™…å­—æ®µï¼‰
                sync_table_structure(table_name, indicators.columns)

            print("é¦–æ¬¡è¿è¡Œè¡¨ç»“æ„åŒæ­¥å®Œæˆ")
        '''



        batch_size = 500
        max_workers = 10

        with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # with ThreadPoolExecutor(max_workers=max_workers) as executor:
            for data_type in ['etf', 'index', 'stock']:
                codes = get_latest_codes(data_type)
                print(f"å¼€å§‹å¤„ç† {data_type} å…± {len(codes)} ä¸ªä»£ç ")

                for batch_idx in range(0, len(codes), batch_size):
                    batch_codes = codes[batch_idx:batch_idx + batch_size]
                    print(f"å¤„ç†æ‰¹æ¬¡ {batch_idx//batch_size+1}ï¼Œä»£ç æ•°ï¼š{len(batch_codes)}")

                    # 1. è·å–æœ¬æ‰¹æ¬¡å†å²æ•°æ®
                    batch_data = get_hist_data_batch(batch_codes, data_type)
                    if batch_data.empty:
                        print(f"æ‰¹æ¬¡ {batch_idx//batch_size+1} æ— æ•°æ®ï¼Œè·³è¿‡")
                        continue

                    # 2. æ‰¹é‡è·å–æœ€åå¤„ç†æ—¥æœŸï¼ˆå…³é”®ä¿®æ”¹ç‚¹ï¼‰
                    # ä»æœ¬æ‰¹æ•°æ®ä¸­æå–æ‰€æœ‰å”¯ä¸€ä»£ç 
                    unique_codes_in_batch = batch_data['code_int'].unique().tolist()
                    last_dates_map = get_last_processed_dates_batch(
                        INDICATOR_TABLES[data_type],
                        unique_codes_in_batch
                    )

                    # 3. å¹¶è¡Œå¤„ç†æœ¬æ‰¹æ¬¡ä»£ç 
                    futures = []
                    for code in batch_codes:
                        # ä»æ‰¹æ¬¡æ•°æ®ä¸­æå–å•ä¸ªä»£ç æ•°æ®
                        code_data = batch_data[batch_data['code_int'] == code].copy()
                        if code_data.empty:
                            continue
                        # æäº¤ä»»åŠ¡æ—¶ä¼ å…¥é¢„å–çš„æœ€åå¤„ç†æ—¥æœŸ
                        futures.append(executor.submit(
                            process_single_code,
                            code=code,
                            data_type=data_type,
                            code_data=code_data,
                            last_processed_date=last_dates_map.get(code, None)
                        ))

                    # 4. åˆå¹¶å¹¶æäº¤æœ¬æ‰¹æ¬¡ç»“æœ
                    valid_dfs = []
                    for future in as_completed(futures):
                        df = future.result()
                        if df is not None and not df.empty:
                            valid_dfs.append(df)

                    if valid_dfs:
                        combined_data = pd.concat(valid_dfs, ignore_index=True)
                        sync_and_save(INDICATOR_TABLES[data_type], combined_data)
                        print(f"æ‰¹æ¬¡æäº¤æˆåŠŸï¼Œè®°å½•æ•°ï¼š{len(combined_data)}")
                    else:
                        print(f"æœ¬æ‰¹æ¬¡æ— æœ‰æ•ˆæ•°æ®")
    finally:
        print(f"\nğŸ•’ æ€»è€—æ—¶: {time.time()-start_time:.2f}ç§’")  # ç¡®ä¿å¼‚å¸¸æ—¶ä¹Ÿè¾“å‡º

if __name__ == "__main__":
    main()
